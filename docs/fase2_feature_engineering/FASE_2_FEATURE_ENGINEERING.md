# ğŸ¨ FASE 2 - STEP 2: FEATURE ENGINEERING âœ…

**Fecha:** 2025-11-12  
**Estado:** COMPLETADO Y FUNCIONAL

---

## ğŸ“¦ MÃ³dulo Implementado

### `src/features/engineer.py`

Clase especializada en ingenierÃ­a de caracterÃ­sticas (selecciÃ³n y creaciÃ³n).

---

## âœ¨ Funcionalidades

### 1. AnÃ¡lisis de CorrelaciÃ³n
```python
engineer.calculate_correlation_matrix(X)
â†’ Matriz de Pearson
â†’ Detecta correlaciones altas (r > 0.8)
â†’ Identifica features redundantes
```

### 2. DetecciÃ³n de Multicolinealidad (VIF)
```python
engineer.calculate_vif(X)
â†’ Variance Inflation Factor por columna
â†’ Identifica features con multicolinealidad (VIF > 10)
â†’ InformaciÃ³n detallada de problemas
```

### 3. SelecciÃ³n Univariante
```python
engineer.select_features_univariate(X, y, k=15, task='classification')
â†’ F-score para clasificaciÃ³n
â†’ F-score para regresiÃ³n
â†’ Top-K features mÃ¡s importantes
â†’ InformaciÃ³n de scores
```

### 4. SelecciÃ³n por InformaciÃ³n Mutua
```python
engineer.select_features_mutual_info(X, y, k=20)
â†’ InformaciÃ³n mutua entre X e y
â†’ Independiente de tipo de relaciÃ³n
â†’ Complementa selecciÃ³n univariante
```

### 5. Remover Features de Baja Varianza
```python
engineer.remove_low_variance_features(X, threshold=0.01)
â†’ Elimina features con varianza < threshold
â†’ Mejora eficiencia del modelo
â†’ Reporta features removidas
```

### 6. Crear Features de InteracciÃ³n
```python
engineer.create_interaction_features(X, limit=10)
â†’ Multiplica pares de features
â†’ Captura relaciones no lineales
â†’ Limitado para eficiencia
```

### 7. Crear Features de RazÃ³n
```python
engineer.create_ratio_features(X, limit=5)
â†’ Divide features relacionadas
â†’ Extrae informaciÃ³n relativa
â†’ Evita divisiÃ³n por cero
```

---

## ğŸ”§ IntegraciÃ³n con Pipeline

### En `src/pipeline.py`

```python
class MLPipeline:
    def engineer_features(self):
        """IngenierÃ­a de caracterÃ­sticas"""
        self.feature_engineer = create_feature_engineer(self.config)
        
        # AnÃ¡lisis
        corr_matrix = engineer.calculate_correlation_matrix(X_train)
        vif = engineer.calculate_vif(X_train)
        
        # SelecciÃ³n
        selected = engineer.select_features_univariate(X_train, y_train)
        
        # Filtrado
        X_filtered = engineer.remove_low_variance_features(X_train)
```

---

## ğŸ“Š Resultados de Prueba

```
Feature Engineering:
  â€¢ Matriz de correlaciÃ³n: (40, 40)
  â€¢ VIF calculado: 40 features
  â€¢ Top 15 features seleccionados
  â€¢ Features de baja varianza removidos: 1
  â€¢ Features despuÃ©s de filtrado: 39

DetecciÃ³n de Correlaciones:
  â€¢ Pares correlacionados (r > 0.8): 0
  â€¢ Sin redundancia detectada

DetecciÃ³n de Multicolinealidad:
  â€¢ Features con VIF > 10: 0
  â€¢ Sin problemas significativos

SelecciÃ³n Univariante:
  â€¢ Features evaluados: 40
  â€¢ Features seleccionados: 15
  â€¢ MÃ©todo: F-score (clasificaciÃ³n)
```

---

## ğŸš€ Uso

### OpciÃ³n 1: Directo

```python
from src.features.engineer import create_feature_engineer
from src.config import Config

config = Config()
engineer = create_feature_engineer(config)

# AnÃ¡lisis
corr = engineer.calculate_correlation_matrix(X)
vif = engineer.calculate_vif(X)

# SelecciÃ³n
selected = engineer.select_features_univariate(
    X, y, k=15, task='classification'
)

# Filtrado
X_clean = engineer.remove_low_variance_features(X)
```

### OpciÃ³n 2: Desde Pipeline

```python
from src.pipeline import MLPipeline

pipeline = MLPipeline()
pipeline.load_data()
pipeline.preprocess_data()
pipeline.engineer_features()

# Acceder a datos
X_train = pipeline.X_train
X_test = pipeline.X_test
engineer_info = pipeline.feature_engineer.get_feature_summary()
```

### OpciÃ³n 3: CLI

```bash
python main.py --mode feature_engineering
# Ejecuta preprocesamiento + ingenierÃ­a
```

---

## ğŸ“‹ MÃ©todos Disponibles

| MÃ©todo | DescripciÃ³n | Input |
|--------|-------------|-------|
| `calculate_correlation_matrix()` | Matriz de correlaciÃ³n | X |
| `calculate_vif()` | Variance Inflation Factor | X |
| `select_features_univariate()` | SelecciÃ³n F-score | X, y, k, task |
| `select_features_mutual_info()` | SelecciÃ³n informaciÃ³n mutua | X, y, k |
| `remove_low_variance_features()` | Filtrar baja varianza | X, threshold |
| `create_interaction_features()` | Features de interacciÃ³n | X, limit |
| `create_ratio_features()` | Features de razÃ³n | X, limit |
| `get_feature_summary()` | Resumen de FE | - |

---

## ğŸ”„ Flujo Completo

```
Dataset Preprocesado (173,522 Ã— 40)
         â†“
Calcular Correlaciones
  â€¢ Matriz 40Ã—40
  â€¢ Detectar r > 0.8
         â†“
Calcular VIF
  â€¢ Multicolinealidad
  â€¢ VIF por feature
         â†“
SelecciÃ³n Univariante (F-score)
  â€¢ Rank features
  â€¢ Top 15 seleccionados
         â†“
Remover Baja Varianza
  â€¢ Threshold: 0.01
  â€¢ 1 feature removido
         â†“
Dataset Optimizado (173,522 Ã— 39)
  â€¢ Listo para modelos
  â€¢ Sin redundancia
  â€¢ Sin baja varianza
         â†“
Modelos âœ“
```

---

## ğŸ¯ AnÃ¡lisis Detallado

### Correlaciones

- **MÃ©todo:** Pearson
- **Umbral:** 0.8 (r > 0.8)
- **Detectadas:** 0 pares
- **ImplicaciÃ³n:** No hay features altamente redundantes

### Multicolinealidad (VIF)

- **MÃ©todo:** Variance Inflation Factor
- **Umbral:** 10
- **Problemas:** 0 features
- **ImplicaciÃ³n:** Baja multicolinealidad

### SelecciÃ³n Univariante

- **MÃ©todo:** F-test (ANOVA)
- **Features evaluados:** 40
- **Features seleccionados:** 15
- **Criterio:** Mayor score F

### Varianza

- **Threshold:** 0.01
- **Features removidos:** 1
- **ImplicaciÃ³n:** Mejora eficiencia

---

## âœ… ValidaciÃ³n

### Test 1: MÃ³dulo Individual
```bash
python src/features/engineer.py
âœ“ Funciona correctamente
âœ“ Genera anÃ¡lisis completo
```

### Test 2: Desde Pipeline
```python
pipeline = MLPipeline()
pipeline.load_data()
pipeline.preprocess_data()
pipeline.engineer_features()
âœ“ IntegraciÃ³n correcta
âœ“ Sin errores
```

### Test 3: MÃ©todos EspecÃ­ficos
```python
engineer = create_feature_engineer(config)
corr = engineer.calculate_correlation_matrix(X)
vif = engineer.calculate_vif(X)
selected = engineer.select_features_univariate(X, y)
âœ“ Todos funcionan
```

---

## ğŸ“Š EstadÃ­sticas

```
Entrada:
  â€¢ Registros: 173,522
  â€¢ Features: 40
  â€¢ Tipo: NumÃ©ricas escaladas + categÃ³ricas codificadas

Salida:
  â€¢ Registros: 173,522 (sin cambios)
  â€¢ Features: 39
  â€¢ Tipo: Optimizadas
  
Cambios:
  â€¢ Features removidos: 1 (varianza baja)
  â€¢ Features creados: 0 (en esta ejecuciÃ³n)
  â€¢ Redundancia detectada: 0
  â€¢ Multicolinealidad: 0
```

---

## ğŸ¯ PrÃ³ximos Pasos

### COMPLETADO âœ…
- [x] Cargar datos
- [x] EDA
- [x] Preprocesamiento
- [x] Feature Engineering

### PRÃ“XIMO ğŸ“
- [ ] Entrenar modelos clasificaciÃ³n
- [ ] Entrenar modelos regresiÃ³n
- [ ] EvaluaciÃ³n
- [ ] Interpretabilidad (XAI)

---

## ğŸ“ Archivos

```
src/features/
â”œâ”€â”€ __init__.py
â””â”€â”€ engineer.py                 âœ… CREADO

src/pipeline.py                 âœ… ACTUALIZADO
  â€¢ engineer_features()
  â€¢ Imports

main.py                          âœ… COMPATIBLE
  â€¢ --mode feature_engineering
```

---

## âœ… Checklist

- [x] Crear clase FeatureEngineer
- [x] Implementar anÃ¡lisis de correlaciÃ³n
- [x] Implementar VIF
- [x] Implementar selecciÃ³n univariante
- [x] Implementar filtrado de varianza
- [x] Implementar creaciÃ³n de features
- [x] Integrar con pipeline
- [x] Probar funcionamiento
- [x] DocumentaciÃ³n

---

## ğŸ”¬ MÃ©todos de SelecciÃ³n

### 1. Univariante (F-test)
- **Caso:** Features numÃ©ricas vs target
- **Ventaja:** RÃ¡pido, simple
- **Desventaja:** No detecta interacciones
- **Uso:** Screening inicial

### 2. InformaciÃ³n Mutua
- **Caso:** Features categÃ³ricas y numÃ©ricas
- **Ventaja:** Captura relaciones no lineales
- **Desventaja:** MÃ¡s computacionalmente costoso
- **Uso:** Complementar univariante

### 3. Varianza
- **Caso:** Features sin variaciÃ³n
- **Ventaja:** Mejora eficiencia
- **Desventaja:** Puede perder info
- **Uso:** Pre-filtrado

---

**FASE 2 - PASO 2: COMPLETADO** âœ…

**PrÃ³ximo:** Entrenar Modelos (ClasificaciÃ³n + RegresiÃ³n)

