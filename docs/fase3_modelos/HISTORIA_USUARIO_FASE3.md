# Historia de Usuario: Fase 3 - Modelado Predictivo

## Contexto
Como **Equipo de Data Science**, necesitamos implementar modelos predictivos para clasificar el estado de titulaci√≥n de estudiantes, bas√°ndonos en las variables ingenierizadas en la Fase 2.

**Estado actual**: Datos preprocesados y features construidas  
**Objetivo**: Modelos entrenados, evaluados y listos para producci√≥n

---

## üìã Historia de Usuario

### ID: FASE3-001
**T√≠tulo**: Desarrollo de modelos predictivos para clasificaci√≥n de titulaci√≥n

**Como** cient√≠fico de datos  
**Quiero** entrenar, evaluar y seleccionar los mejores modelos predictivos  
**Para** poder hacer predicciones precisas sobre el estado de titulaci√≥n de estudiantes

**Contexto**: El dataset ya est√° preprocesado (Fase 1-2), features ingenierizadas, normalizado y dividido en train/test.

---

## ‚úÖ Criterios de Aceptaci√≥n (Gherkin)

### Escenario 1: Entrenamiento de modelos base
```gherkin
Escenario: Entrenar m√∫ltiples algoritmos clasificadores
  Dado que tengo el dataset preprocesado en "data/processed/final_dataset.csv"
  Y tengo features seleccionadas documentadas en "src/config/features_config.yml"
  
  Cuando entreno los siguientes modelos:
    - Logistic Regression
    - Random Forest
    - Gradient Boosting
    - SVM
    - Neural Network
  
  Entonces cada modelo debe:
    ‚úì Converger sin errores
    ‚úì Generar m√©tricas base (Accuracy, Precision, Recall, F1)
    ‚úì Ser guardado en "models/trained/[model_name]_v1.pkl"
    ‚úì Registrar sus hiperpar√°metros en "models/metadata/training_log.json"
```

### Escenario 2: Evaluaci√≥n y validaci√≥n cruzada
```gherkin
Escenario: Validar modelos con K-Fold Cross-Validation
  Dado que tengo 5 modelos entrenados
  Y utilizo 5-Fold Cross-Validation
  
  Cuando eval√∫o cada modelo
  
  Entonces debo obtener:
    ‚úì Scores de CV con desviaci√≥n est√°ndar < 5%
    ‚úì Matriz de confusi√≥n por clase
    ‚úì Curva ROC-AUC
    ‚úì Reporte de clasificaci√≥n completo
    ‚úì Tabla comparativa de modelos en "outputs/model_comparison.html"
```

### Escenario 3: Selecci√≥n del mejor modelo
```gherkin
Escenario: Identificar modelo √≥ptimo
  Dado que tengo m√©tricas de evaluaci√≥n de 5 modelos
  Y los criterios de selecci√≥n son: F1-Score (60%), Recall (30%), Latencia (10%)
  
  Cuando aplico ponderaci√≥n de criterios
  
  Entonces:
    ‚úì Modelo ganador tiene F1 > 0.75
    ‚úì Recall > 0.70 (minimizar falsos negativos)
    ‚úì Latencia inferencia < 100ms
    ‚úì Se exporta como "models/production/best_model_v1.pkl"
```

### Escenario 4: An√°lisis de importancia de features
```gherkin
Escenario: Entender contribuci√≥n de features
  Dado que tengo el modelo seleccionado
  
  Cuando calculo importancia de features
  
  Entonces:
    ‚úì Top 15 features identificados
    ‚úì Gr√°fico SHAP exportado a "outputs/feature_importance.png"
    ‚úì Gr√°fico Permutation Importance generado
    ‚úì An√°lisis guardado en "docs/fase3_modelos/ANALISIS_FEATURES.md"
```

---

## üîß Tareas de Programaci√≥n

### Sprint 1: Setup y Modelos Base

**TAREA-3.1**: Crear estructura de training
```python
# src/models/training.py
- Funci√≥n: load_data_split(test_size=0.2, val_size=0.1)
- Funci√≥n: get_base_models() -> dict
- Funci√≥n: train_model(model, X_train, y_train) -> trained_model
- Logging: Registrar tiempos de entrenamiento y recursos
```

**TAREA-3.2**: Implementar Logistic Regression + Random Forest
```python
# src/models/classifiers.py
- LR: parametrizaci√≥n (C, solver, max_iter)
- RF: parametrizaci√≥n (n_estimators, max_depth, min_samples)
- Grid Search b√°sico para cada uno
- Exportaci√≥n de modelos entrenados
```

**TAREA-3.3**: Implementar Gradient Boosting + SVM
```python
# src/models/advanced_models.py
- GB: XGBoost o LightGBM con tuning
- SVM: Kernel selection + C parameter
- Validaci√≥n cruzada K=5
```

### Sprint 2: Evaluaci√≥n y Comparaci√≥n

**TAREA-3.4**: M√≥dulo de evaluaci√≥n
```python
# src/models/evaluation.py
- Funci√≥n: evaluate_model(y_true, y_pred) -> MetricsDict
- Funci√≥n: cross_validate_models(models, X, y, k=5)
- Funci√≥n: generate_confusion_matrix(y_true, y_pred)
- Funci√≥n: plot_roc_curves(models_results)
```

**TAREA-3.5**: Dashboard de comparaci√≥n
```python
# notebooks/03_MODEL_EVALUATION.ipynb
- Tabla comparativa con m√©tricas normalizadas
- Gr√°ficos de rendimiento lado a lado
- Matriz de correlaci√≥n de predicciones
- Exportar resumen en HTML interactivo
```

**TAREA-3.6**: An√°lisis de importancia
```python
# src/models/interpretability.py
- Funci√≥n: calculate_feature_importance(model, X)
- Funci√≥n: plot_shap_values(model, X)
- Funci√≥n: permutation_importance(model, X_test, y_test)
```

### Sprint 3: Selecci√≥n y Producci√≥n

**TAREA-3.7**: Mecanismo de selecci√≥n
```python
# src/models/model_selection.py
- Funci√≥n: weighted_score(metrics, weights)
- Funci√≥n: select_best_model(models_results, criteria)
- Exportar campe√≥n a models/production/
```

**TAREA-3.8**: Validaci√≥n en test set
```python
# notebooks/04_FINAL_VALIDATION.ipynb
- Predicciones en test set virgen
- Reporte final de performance
- Comparaci√≥n train vs test (detectar overfitting)
- Umbral de decisi√≥n √≥ptimo
```

**TAREA-3.9**: Documentaci√≥n de modelos
```python
# docs/fase3_modelos/MODELOS_FINALES.md
- Especificaciones t√©cnicas de cada modelo
- Hiperpar√°metros √≥ptimos
- Performance metrics finales
- Recomendaciones de uso
```

---

## üìä Entregables Esperados

```
outputs/
‚îú‚îÄ‚îÄ model_comparison.html          # Tabla interactiva
‚îú‚îÄ‚îÄ feature_importance.png         # Top 15 features
‚îú‚îÄ‚îÄ confusion_matrices.png         # 2x3 subplot
‚îú‚îÄ‚îÄ roc_curves.png                 # Todas las curvas
‚îî‚îÄ‚îÄ model_performance_report.pdf   # Resumen ejecutivo

models/
‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îî‚îÄ‚îÄ best_model_v1.pkl         # Modelo ganador
‚îú‚îÄ‚îÄ trained/
‚îÇ   ‚îú‚îÄ‚îÄ lr_v1.pkl
‚îÇ   ‚îú‚îÄ‚îÄ rf_v1.pkl
‚îÇ   ‚îú‚îÄ‚îÄ gb_v1.pkl
‚îÇ   ‚îú‚îÄ‚îÄ svm_v1.pkl
‚îÇ   ‚îî‚îÄ‚îÄ nn_v1.pkl
‚îî‚îÄ‚îÄ metadata/
    ‚îú‚îÄ‚îÄ training_log.json
    ‚îî‚îÄ‚îÄ model_cards/
        ‚îî‚îÄ‚îÄ best_model_v1_card.md

notebooks/
‚îú‚îÄ‚îÄ 03_MODEL_EVALUATION.ipynb      # Comparaci√≥n de modelos
‚îî‚îÄ‚îÄ 04_FINAL_VALIDATION.ipynb      # Validaci√≥n final

docs/fase3_modelos/
‚îú‚îÄ‚îÄ MODELOS_FINALES.md             # Especificaciones
‚îî‚îÄ‚îÄ ANALISIS_FEATURES.md           # Importancia de features
```

---

## üéØ Criterios de √âxito

| Criterio | Umbral | Prioridad |
|----------|--------|-----------|
| F1-Score (Test Set) | > 0.75 | üî¥ Alta |
| Recall (clase minoritaria) | > 0.70 | üî¥ Alta |
| Overfitting (|train_f1 - test_f1|) | < 0.05 | üü° Media |
| Latencia predicci√≥n | < 100ms | üü¢ Baja |
| Reproducibilidad (seed fijo) | Determin√≠stico | üü° Media |

---

## üìù Notas

- **Balanceo de clases**: Usar SMOTE si hay desbalance significativo
- **Feature scaling**: Ya aplicado en Fase 2, verificar en training
- **Baseline**: Iniciar con dummy classifier para comparaci√≥n
- **Hyperparameter tuning**: Usar Optuna o GridSearchCV
- **Reproducibilidad**: Fijar random_state=42 en todos los modelos

---

## üîó Dependencias

- ‚úÖ Fase 1: EDA completado
- ‚úÖ Fase 2: Feature Engineering completado
- üì¶ Requerimientos: sklearn, xgboost, tensorflow, shap, optuna
