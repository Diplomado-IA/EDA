#!/usr/bin/env python3
"""
PASO 2: ENTRENAR MODELOS DE CLASIFICACI√ìN (MODALIDAD)
======================================================

Script para entrenar y evaluar modelos de clasificaci√≥n en Fase 3.
"""

import logging
import sys
import pandas as pd
import numpy as np
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

def main():
    logger.info("\n" + "="*80)
    logger.info("‚úÖ PASO 2: ENTRENAR MODELOS DE CLASIFICACI√ìN")
    logger.info("="*80)

    # Verificar que los datos de Fase 2 est√©n disponibles
    data_path = Path("data/processed")
    required_files = [
        "X_train_engineered.pkl",
        "X_test_engineered.pkl",
        "y_train_classification.pkl",
        "y_test_classification.pkl"
    ]

    logger.info("\nüìã Verificando archivos requeridos...")
    all_exist = True
    for file in required_files:
        filepath = data_path / file
        exists = filepath.exists()
        status = "‚úì" if exists else "‚úó"
        logger.info(f"  {status} {file}")
        if not exists:
            all_exist = False

    if not all_exist:
        logger.error("\n‚ùå ERROR: Faltan archivos de Fase 2")
        logger.info("\nArchivos esperados en data/processed/:")
        for file in required_files:
            logger.info(f"  - {file}")
        return False

    logger.info("\n‚úÖ Todos los archivos requeridos encontrados")

    # Cargar datos
    logger.info("\nüìÇ Cargando datos...")
    try:
        X_train = pd.read_pickle(data_path / "X_train_engineered.pkl")
        X_test = pd.read_pickle(data_path / "X_test_engineered.pkl")
        y_train_class = pd.read_pickle(data_path / "y_train_classification.pkl")
        y_test_class = pd.read_pickle(data_path / "y_test_classification.pkl")
        
        logger.info(f"  ‚úì X_train: {X_train.shape}")
        logger.info(f"  ‚úì X_test: {X_test.shape}")
        logger.info(f"  ‚úì y_train: {y_train_class.shape}")
        logger.info(f"  ‚úì y_test: {y_test_class.shape}")
    except Exception as e:
        logger.error(f"\n‚ùå Error cargando datos: {e}")
        return False

    # Verificar distribuci√≥n de clases
    logger.info("\nüìä Distribuci√≥n de clases (Train):")
    value_counts = y_train_class.value_counts()
    for clase, count in value_counts.items():
        pct = (count / len(y_train_class)) * 100
        logger.info(f"  {clase}: {count:,} ({pct:.1f}%)")

    logger.info("\nüìä Distribuci√≥n de clases (Test):")
    value_counts = y_test_class.value_counts()
    for clase, count in value_counts.items():
        pct = (count / len(y_test_class)) * 100
        logger.info(f"  {clase}: {count:,} ({pct:.1f}%)")

    # Importar m√≥dulo de modelos
    logger.info("\nüîß Importando m√≥dulo de modelos...")
    try:
        from src.models import ModelTrainer, ModelEvaluator
        logger.info("  ‚úì ModelTrainer importado")
        logger.info("  ‚úì ModelEvaluator importado")
    except Exception as e:
        logger.error(f"\n‚ùå Error importando m√≥dulo: {e}")
        return False

    # ===== ENTRENAR CLASIFICACI√ìN =====
    logger.info("\nü§ñ Inicializando entrenador...")
    trainer = ModelTrainer(random_state=42)

    logger.info("\n‚è±Ô∏è  Entrenando modelos de clasificaci√≥n...")
    try:
        models_class = trainer.train_classification(X_train, y_train_class)
    except Exception as e:
        logger.error(f"\n‚ùå Error entrenando modelos: {e}")
        return False

    logger.info("\n‚è±Ô∏è  Realizando validaci√≥n cruzada (5-Fold)...")
    try:
        cv_results_class = trainer.cross_validate_classification(X_train, y_train_class, k=5)
    except Exception as e:
        logger.error(f"\n‚ùå Error en validaci√≥n cruzada: {e}")
        return False

    # ===== EVALUAR CLASIFICACI√ìN =====
    logger.info("\nüìä Inicializando evaluador...")
    evaluator = ModelEvaluator()

    logger.info("\n‚è±Ô∏è  Evaluando modelos en test set...")
    try:
        results_class = evaluator.evaluate_classification(models_class, X_test, y_test_class)
    except Exception as e:
        logger.error(f"\n‚ùå Error evaluando modelos: {e}")
        return False

    # ===== GUARDAR MODELOS Y RESULTADOS =====
    logger.info("\nüíæ Guardando modelos...")
    try:
        saved_models = trainer.save_models(output_dir="models/trained")
    except Exception as e:
        logger.error(f"\n‚ùå Error guardando modelos: {e}")
        return False

    logger.info("\nüíæ Guardando resultados...")
    try:
        saved_results = evaluator.save_results(output_dir="models/metadata")
    except Exception as e:
        logger.error(f"\n‚ùå Error guardando resultados: {e}")
        return False

    # ===== RESUMEN FINAL =====
    logger.info("\n" + "="*80)
    logger.info("‚úÖ PASO 2 COMPLETADO: CLASIFICACI√ìN")
    logger.info("="*80)

    logger.info("\nüìä RESULTADOS EN TEST SET:")
    logger.info("\n" + "-"*80)
    for model_name, metrics in results_class.items():
        logger.info(f"\nüîç Modelo: {model_name.upper()}")
        logger.info(f"  ‚Ä¢ Accuracy:  {metrics['accuracy']:.4f}")
        logger.info(f"  ‚Ä¢ Precision: {metrics['precision']:.4f}")
        logger.info(f"  ‚Ä¢ Recall:    {metrics['recall']:.4f}")
        logger.info(f"  ‚Ä¢ F1-Score:  {metrics['f1']:.4f}")
        if metrics.get('roc_auc'):
            logger.info(f"  ‚Ä¢ ROC-AUC:   {metrics['roc_auc']:.4f}")

    logger.info("\n" + "-"*80)
    logger.info("\nüìÇ Archivos Guardados:")
    for key, path in saved_models.items():
        logger.info(f"  ‚úì {Path(path).name}")
    for key, path in saved_results.items():
        logger.info(f"  ‚úì {Path(path).name}")

    logger.info("\n" + "="*80)
    logger.info("üéØ PR√ìXIMO: PASO 3 - Entrenar Regresi√≥n")
    logger.info("="*80 + "\n")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
